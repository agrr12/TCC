{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agrr12/TCC/blob/master/TCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L9wIOsMbnns_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f450213-5e38-4487-87c7-a236258a7812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.sql.functions import *\n",
        "from google.colab import drive\n",
        "from pyspark.sql.types import *\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from datetime import *\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from matplotlib.ticker import MaxNLocator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmnN-hNH083J"
      },
      "source": [
        "# Definition of Auxiliary Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM4x0A_b4A7b"
      },
      "source": [
        "## 1. create_sparse_matrix\n",
        "- **Purpose**: Creates a sparse matrix based on the difference between two date strings.\n",
        "- **Description**: Returns a matrix with rows as days and columns corresponding to the desired unit (seconds, minutes, or hours)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LVvsHz5sg3PP"
      },
      "outputs": [],
      "source": [
        "def create_sparse_matrix(start_date_str: str, end_date_str: str, unit: str = 'seconds') -> sparse.dok_matrix:\n",
        "    \"\"\"\n",
        "    Create a sparse matrix based on the difference between two date strings.\n",
        "\n",
        "    Parameters:\n",
        "    - start_date_str (str): Start date in the format '%Y-%m-%d %H:%M:%S'.\n",
        "    - end_date_str (str): End date in the format '%Y-%m-%d %H:%M:%S'.\n",
        "    - unit (str): Desired unit for the matrix columns ('seconds', 'minutes', or 'hours').\n",
        "\n",
        "    Returns:\n",
        "    - sparse.dok_matrix: A matrix with shape (days, units_per_day).\n",
        "    \"\"\"\n",
        "    # Define the date format\n",
        "    fmt = '%Y-%m-%d %H:%M:%S'\n",
        "\n",
        "    # Parse the start and end dates\n",
        "    d1 = datetime.strptime(start_date_str, fmt)\n",
        "    d2 = datetime.strptime(end_date_str, fmt)\n",
        "\n",
        "    # Calculate the difference in days\n",
        "    delta = d2 - d1\n",
        "    days = delta.days + 1  # +1 to include the last day\n",
        "\n",
        "    # Define units per day\n",
        "    if unit == 'seconds':\n",
        "        units_per_day = 24 * 60 * 60\n",
        "    elif unit == 'minutes':\n",
        "        units_per_day = 24 * 60\n",
        "    elif unit == 'hours':\n",
        "        units_per_day = 24\n",
        "\n",
        "    # Create a sparse matrix with zeros\n",
        "    sparse_matrix = sparse.dok_matrix((days, units_per_day), dtype=np.int8)\n",
        "\n",
        "    return sparse_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Jta0L54CPz"
      },
      "source": [
        "## 2. generate_image\n",
        "- **Purpose**: Visualizes date-time occurrences from a sparse matrix.\n",
        "- **Description**: Processes date-time objects, updates the sparse matrix based on occurrences, and then creates an image (either hexbin or scatter) to represent the data visually. The resultant image is saved in a specified directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m0t5EOkb10_J"
      },
      "outputs": [],
      "source": [
        "def generate_image(dates, img_num, unit= 'seconds', sparse_matrix= None, image_type='hexbin', start_date_str = \"\", grid=10):\n",
        "    folder = f'days_{unit}'\n",
        "    # Loop through each date and update the matrix based on the desired unit\n",
        "    for d in dates:\n",
        "        day = (datetime.strptime(d, '%Y-%m-%d %H:%M:%S') - datetime.strptime(start_date_str, '%Y-%m-%d %H:%M:%S')).days\n",
        "        datetime_d = datetime.strptime(d, '%Y-%m-%d %H:%M:%S')\n",
        "        if unit == 'seconds':\n",
        "            index = datetime_d.hour * 3600 + datetime_d.minute * 60 + datetime_d.second\n",
        "        elif unit == 'minutes':\n",
        "            index = datetime_d.hour * 60 + datetime_d.minute\n",
        "        elif unit == 'hours':\n",
        "            index = datetime_d.hour\n",
        "        sparse_matrix[day-1, index-1] += 1\n",
        "\n",
        "    if image_type == 'hexbin':\n",
        "      dense_matrix = sparse_matrix.toarray()\n",
        "      # Get the x and y coordinates\n",
        "      a, b = np.indices(dense_matrix.shape)\n",
        "      matrix_flat = dense_matrix.flatten()\n",
        "      a_flat = a.flatten()\n",
        "      b_flat = b.flatten()\n",
        "      plt.hexbin(b_flat, a_flat, gridsize=grid, cmap='inferno', C=matrix_flat)\n",
        "    elif image_type == 'scatter':\n",
        "      if unit == 'seconds':\n",
        "          plt.xlim(-10000,100000)\n",
        "          plt.ylim(-10, 100)\n",
        "      elif unit == 'minutes':\n",
        "          plt.xlim(-100,1500)\n",
        "          plt.ylim(-10, 100)\n",
        "      elif unit == 'hours':\n",
        "          plt.xlim(-5,26)\n",
        "          plt.ylim(-10, 100)\n",
        "      # Get the non-zero entries of the sparse matrix\n",
        "      nonzero_rows, nonzero_cols = sparse_matrix.nonzero()\n",
        "      plt.scatter(nonzero_cols, nonzero_rows, marker='s', s=80, alpha=0.2, color='red')\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f'/content/drive/My Drive/TCC/images_dir/{folder}/{image_type}/{img_num}.png')\n",
        "    #plt.show()\n",
        "    plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjEsPXbi4IJx"
      },
      "source": [
        "## 3. timestamp_frequency\n",
        "- **Purpose**: Calculates the frequency of timestamps in a specified range.\n",
        "- **Description**: Computes the frequency of timestamps within the period \"2022-09-01 00:00:01\" to \"2022-11-30 23:59:59\". The result is a list where each index indicates the frequency at that timestamp, adjusted based on the given granularity (seconds, minutes, or hours).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xJHS5a7zCooC"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "from collections import Counter\n",
        "\n",
        "def timestamp_frequency(timestamps, granularity='seconds'):\n",
        "    # Define the start and end timestamps based on the provided format\n",
        "    start_timestamp = datetime.strptime(\"2022-09-01 00:00:01\", \"%Y-%m-%d %H:%M:%S\")\n",
        "    end_timestamp = datetime.strptime(\"2022-11-30 23:59:59\", \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Adjustments based on granularity\n",
        "    if granularity == 'minutes':\n",
        "        start_timestamp = start_timestamp.replace(second=0)\n",
        "        end_timestamp = end_timestamp.replace(second=59)\n",
        "        timestamps = [ts if isinstance(ts, datetime) else datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S\").replace(second=0) for ts in timestamps]\n",
        "        delta = timedelta(minutes=1)\n",
        "    elif granularity == 'hours':\n",
        "        start_timestamp = start_timestamp.replace(minute=0, second=0)\n",
        "        end_timestamp = end_timestamp.replace(minute=59, second=59)\n",
        "        timestamps = [ts if isinstance(ts, datetime) else datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S\").replace(minute=0, second=0) for ts in timestamps]\n",
        "        delta = timedelta(hours=1)\n",
        "    else:  # default is seconds\n",
        "        delta = timedelta(seconds=1)\n",
        "        timestamps = [ts if isinstance(ts, datetime) else datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S\") for ts in timestamps]\n",
        "\n",
        "    # Calculate the total intervals based on granularity in the date range\n",
        "    total_intervals = int((end_timestamp - start_timestamp) / delta) + 1\n",
        "\n",
        "    # Initialize the result list with zeros for the entire range\n",
        "    result = [0] * total_intervals\n",
        "\n",
        "    # Use Counter to count occurrences of each timestamp\n",
        "    timestamp_counts = Counter(timestamps)\n",
        "\n",
        "    # For each unique timestamp, update the corresponding index in the result list\n",
        "    for timestamp, count in timestamp_counts.items():\n",
        "        idx = int((timestamp - start_timestamp) / delta)\n",
        "        result[idx] = count\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHiqYX-24NeL"
      },
      "source": [
        "## 4. run_length_encode_zeros\n",
        "- **Purpose**: Encodes sequences of zeros from a timestamp frequency list.\n",
        "- **Description**: After calculating the timestamp frequency with the given granularity, the function performs run-length encoding focusing on zero sequences. Positive values in the output represent original frequency counts, while negative values represent consecutive runs of zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4nis8RBA153-"
      },
      "outputs": [],
      "source": [
        "def run_length_encode_zeros(array, granularity):\n",
        "    arr = timestamp_frequency(array, granularity)\n",
        "    encoded = []\n",
        "    zero_count = 0\n",
        "\n",
        "    for val in arr:\n",
        "        if val == 0:\n",
        "            zero_count += 1\n",
        "        else:\n",
        "            if zero_count:\n",
        "                encoded.append(-zero_count)\n",
        "                zero_count = 0\n",
        "            encoded.append(val)\n",
        "\n",
        "    # Handle case where the array ends with zeros\n",
        "    if zero_count:\n",
        "        encoded.append(-zero_count)\n",
        "\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-yTq5E-4RgH"
      },
      "source": [
        "## 5. ConstrainedAWARP\n",
        "- **Purpose**: Computes the constrained adaptive time warping distance between two sequences.\n",
        "- **Description**: This function determines the optimal alignment between two time series while considering time shifts and distortions. It uses a dynamic programming approach, constructing a matrix where each cell represents the warping cost up to those elements of both sequences. The overall warping distance is the value in the matrix's bottom-right corner, and this matrix is also returned for detailed insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaGIpqMyrOUh"
      },
      "outputs": [],
      "source": [
        "def ConstrainedAWARP(x, y, w):\n",
        "    # Initialize the lengths of both sequences and append the value '1' to each.\n",
        "    n = len(x)\n",
        "    m = len(y)\n",
        "    x.append(1)\n",
        "    y.append(1)\n",
        "\n",
        "    # Create a 2D array filled with infinity for dynamic programming.\n",
        "    D = np.inf * np.ones((n+1, m+1))\n",
        "    D[0][0] = 0  # Base case\n",
        "\n",
        "    # Calculate the timestamps of the events in x.\n",
        "    # If value is positive, it's an event; if negative, it's a gap.\n",
        "    tx = [0] * (n+1)\n",
        "    iit = 0\n",
        "    for i in range(n+1):\n",
        "        if x[i] > 0:\n",
        "            iit += 1\n",
        "            tx[i] = iit\n",
        "        else:\n",
        "            iit += abs(x[i])\n",
        "            tx[i] = iit\n",
        "\n",
        "    # Similarly, calculate the timestamps of the events in y.\n",
        "    ty = [0] * (m+1)\n",
        "    iit = 0\n",
        "    for i in range(m+1):\n",
        "        if y[i] > 0:\n",
        "            iit += 1\n",
        "            ty[i] = iit\n",
        "        else:\n",
        "            iit += abs(y[i])\n",
        "            ty[i] = iit\n",
        "\n",
        "    # Iterate through both sequences to compute the warping cost.\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            gap = abs(tx[i] - ty[j])\n",
        "\n",
        "            # If the gap between events is larger than w, set to infinity.\n",
        "            if gap > w and ((j > 0 and ty[j-1] - tx[i] > w) or (i > 0 and tx[i-1] - ty[j] > w)):\n",
        "                D[i+1][j+1] = np.inf\n",
        "            else:\n",
        "                # Initialize three possible costs (from diagonal, left, and top).\n",
        "                a1, a2, a3 = np.inf, np.inf, np.inf\n",
        "\n",
        "                # Cost calculation for matching x[i] and y[j].\n",
        "                if x[i] > 0 and y[j] > 0 and gap <= w:\n",
        "                    a1 = D[i][j] + (x[i] - y[j]) ** 2\n",
        "                elif x[i] < 0 and y[j] < 0:\n",
        "                    a1 = D[i][j]\n",
        "                elif x[i] > 0 and y[j] < 0:\n",
        "                    a1 = D[i][j] + x[i] ** 2 * (-y[j])\n",
        "                elif x[i] < 0 and y[j] > 0:\n",
        "                    a1 = D[i][j] + y[j] ** 2 * (-x[i])\n",
        "\n",
        "                # Cost calculation for matching x[i] and a gap in y.\n",
        "                if x[i] > 0 and y[j] > 0 and gap <= w:\n",
        "                    a2 = D[i+1][j] + (x[i] - y[j]) ** 2\n",
        "                elif x[i] < 0 and y[j] < 0:\n",
        "                    a2 = D[i+1][j]\n",
        "                elif x[i] < 0 and y[j] > 0:\n",
        "                    a2 = D[i+1][j] + y[j] ** 2\n",
        "                elif x[i] > 0 and y[j] < 0 and gap <= w:\n",
        "                    a2 = D[i+1][j] + x[i] ** 2 * (-y[j])\n",
        "\n",
        "                # Cost calculation for matching y[j] and a gap in x.\n",
        "                if x[i] > 0 and y[j] > 0 and gap <= w:\n",
        "                    a3 = D[i][j+1] + (x[i] - y[j]) ** 2\n",
        "                elif x[i] < 0 and y[j] < 0:\n",
        "                    a3 = D[i][j+1]\n",
        "                elif x[i] > 0 and y[j] < 0:\n",
        "                    a3 = D[i][j+1] + x[i] ** 2\n",
        "                elif x[i] < 0 and y[j] > 0 and gap <= w:\n",
        "                    a3 = D[i][j+1] + y[j] ** 2 * (-x[i])\n",
        "\n",
        "                # Store the minimum of the three computed costs.\n",
        "                D[i+1][j+1] = min([a1, a2, a3])\n",
        "\n",
        "    # Return the square root of the final cell value as the distance and the matrix D.\n",
        "    d = np.sqrt(D[n][m])\n",
        "    return d, D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1KW-XO206Ue"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6jWbcjT8qjP"
      },
      "source": [
        "#ETL of the Databases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7DBD5P7OntJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d3fdb1-42bc-4ef0-8d3d-bac83a59344a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").appName('TCC').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "file_name = 'basics.csv'\n",
        "drive_path= 'My Drive/TCC'\n",
        "drive.mount('/content/drive')\n",
        "df = spark.read.csv(f'/content/drive/{drive_path}/{file_name}', inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t6UJX22GTdKe"
      },
      "outputs": [],
      "source": [
        "#Deduplicate\n",
        "df_basic = df.dropDuplicates()\n",
        "#Consider only the experiment date range\n",
        "df_basic = df_basic.filter(\"publishedAt <= '2022-11-30 23:59:59'\")\n",
        "#Get number of comments per user\n",
        "df_comments_per_user = df_basic.groupby(\"authorChannelId\").count()\n",
        "#Filter out all those with 500 or less comments\n",
        "df_high_commenters = df_comments_per_user.filter(\"count>500\").withColumnRenamed(\"authorChannelId\", 'authorChannelId_HC')\n",
        "#Build a dataframe with only the comments of those with more than 500 comments\n",
        "high_commenters_dates = df_high_commenters[['authorChannelId_HC']].join(df_basic[['authorChannelId', 'publishedAt']], df_high_commenters.authorChannelId_HC == df_basic.authorChannelId, 'left').drop('authorChannelId_HC')\n",
        "#Build a dataframe with one row per user, where the row contains the user ID and a list of all the dates of the comments published by the user\n",
        "dates_as_arrays = high_commenters_dates.groupby(\"authorChannelId\").agg(collect_list(col(\"publishedAt\").cast(\"string\")))\n",
        "#dates_as_arrays.toPandas().to_csv('/content/drive/My Drive/TCC/date_array.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqAe8qA09HC7"
      },
      "source": [
        "#Creation of Dataset with encoded time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eohQ03m-9KMx"
      },
      "outputs": [],
      "source": [
        "# UDF without partially applied arguments\n",
        "udf_run_length_encode_zeros_seconds = udf(lambda x: run_length_encode_zeros(x, 'seconds'), ArrayType(IntegerType()))\n",
        "udf_run_length_encode_zeros_minutes = udf(lambda x: run_length_encode_zeros(x, 'minutes'), ArrayType(IntegerType()))\n",
        "udf_run_length_encode_zeros_hours = udf(lambda x: run_length_encode_zeros(x, 'hours'), ArrayType(IntegerType()))\n",
        "\n",
        "# Apply UDF to DataFrame\n",
        "df_sec = dates_as_arrays.withColumn(\"encode_sec\", udf_run_length_encode_zeros_seconds(dates_as_arrays[\"collect_list(publishedAt)\"])).drop(\"collect_list(publishedAt)\")\n",
        "df_min = dates_as_arrays.withColumn(\"encode_min\", udf_run_length_encode_zeros_minutes(dates_as_arrays[\"collect_list(publishedAt)\"])).drop(\"collect_list(publishedAt)\")\n",
        "df_hou = dates_as_arrays.withColumn(\"encode_hou\", udf_run_length_encode_zeros_hours(dates_as_arrays[\"collect_list(publishedAt)\"])).drop(\"collect_list(publishedAt)\")\n",
        "\n",
        "df_sec.toPandas().to_csv('/content/drive/My Drive/TCC/encoded_seconds.csv', index=False)\n",
        "df_min.toPandas().to_csv('/content/drive/My Drive/TCC/encoded_minutes.csv', index=False)\n",
        "df_hou.toPandas().to_csv('/content/drive/My Drive/TCC/encoded_hours.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK5rn6a99KJ4"
      },
      "outputs": [],
      "source": [
        "p = df_sec.withColumn.toPandas()\n",
        "\n",
        "p.to_csv('/content/drive/My Drive/TCC/t.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mol0sKcl7w6F"
      },
      "source": [
        "#Writing lenght-encoded time-series as plots"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.ticker import MaxNLocator\n",
        "s = pd.read_csv(f'/content/drive/{drive_path}/encoded_hours.csv')\n",
        "for x,y in s.iterrows():\n",
        "      author = y['authorChannelId']\n",
        "      serie = y['encode_hou']\n",
        "      plt.plot(eval(serie))\n",
        "      #ax = plt.gca()  # Get current axis\n",
        "      #ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "      #plt.ylabel('Quantidade de Comentários')\n",
        "      #plt.xlabel('Horas desde 2022-09-01 00:00:01')\n",
        "      #plt.title(f'Usuário {author} como Série Temporal')\n",
        "      plt.axis('off')\n",
        "      #plt.show()\n",
        "      plt.savefig(f'/content/drive/My Drive/TCC/images_dir/encoded_plot/hours/{x}.png')\n",
        "      plt.clf()"
      ],
      "metadata": {
        "id": "wAxYKv7RRNn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conversion of Time Series into Images"
      ],
      "metadata": {
        "id": "SxtlV-_RRM6w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5215vZLXA0CF"
      },
      "outputs": [],
      "source": [
        "# Define the schema for the DataFrame\n",
        "#grid seconds- 110\n",
        "#grid minutes - 80\n",
        "#grid hours - 23\n",
        "\n",
        "data = []\n",
        "d = {'seconds': 110, 'minutes': 80, 'hours':23}\n",
        "image_types = ['hexbin', 'scatter']\n",
        "for x,y in dates_as_arrays.toPandas().iterrows():\n",
        "  author = y['authorChannelId']\n",
        "  dates = y['collect_list(CAST(publishedAt AS STRING))']\n",
        "  data.append((author, x))\n",
        "  for unit in d:\n",
        "    folder = f'days_{unit}'\n",
        "    #criar uma matriz esparsa com zeros\n",
        "    sparse_matrix = create_sparse_matrix('2022-09-01 00:00:01', '2022-11-30 23:59:59', unit)\n",
        "    for img_type in image_types:\n",
        "      generate_image(dates, x, unit, sparse_matrix, img_type, '2022-09-01 00:00:01', grid=d[unit])\n",
        "  print(x)\n",
        "\n",
        "\n",
        "df_pandas = pd.DataFrame(data, columns=['authorChannelId', 'imgNumber'])\n",
        "# Write the Pandas DataFrame to a CSV file\n",
        "df_pandas.to_csv(f\"/content/drive/My Drive/TCC/images_dir/image_map.csv\", index=False, sep=\";\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKQaZ1SDf2w5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(f'/content/drive/My Drive/TCC/encoded_hours.csv')\n",
        "\n",
        "\n",
        "for x1,y1 in df[100::].iterrows():\n",
        "  id1 = y1['authorChannelId']\n",
        "  series1 = [int(i) for i in y1['encode_hou'].strip('[]').split(',')]\n",
        "  for x2,y2 in df[int(x1)+1::].iterrows():\n",
        "    id2 = y2['authorChannelId']\n",
        "    series2 = [int(i) for i in y2['encode_hou'].strip('[]').split(',')]\n",
        "    print(id1, id2, ConstrainedAWARP(series1, series2, np.inf)[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "RM4x0A_b4A7b",
        "m7Jta0L54CPz",
        "ZjEsPXbi4IJx",
        "aHiqYX-24NeL",
        "_-yTq5E-4RgH"
      ],
      "authorship_tag": "ABX9TyPrSlsVbNN/YIxZY6has140",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}